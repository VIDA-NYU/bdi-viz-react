import logging
from typing import Any, Dict, Generator, List, Optional

from dotenv import load_dotenv

load_dotenv()
from langchain.output_parsers import PydanticOutputParser
from langchain_core.messages import AIMessage, HumanMessage, SystemMessage

# from langchain_anthropic import ChatAnthropic
# from langchain_ollama import ChatOllama
# from langchain_together import ChatTogether
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.prebuilt import create_react_agent
from pydantic import BaseModel

from ..tools.candidate_butler import CandidateButler
from ..tools.rag_researcher import retrieve_from_rag
from .memory import MemoryRetriver
from .pydantic import (
    ActionResponse,
    AgentSuggestions,
    CandidateExplanation,
    SearchResponse,
)

logger = logging.getLogger("bdiviz_flask.sub")


class Agent:
    def __init__(self) -> None:
        # OR claude-3-5-sonnet-20240620
        # self.llm = ChatAnthropic(model="claude-3-5-sonnet-latest")
        # self.llm = ChatOllama(base_url='https://ollama-asr498.users.hsrn.nyu.edu', model='llama3.1:8b-instruct-fp16', temperature=0.2)
        # self.llm = ChatOllama(model="deepseek-r1:1.5b", temperature=0.2)
        # self.llm = ChatTogether(model="meta-llama/Llama-3.3-70B-Instruct-Turbo")
        self.llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)
        self.agent_config = {"configurable": {"thread_id": "bdiviz-1"}}

        # self.memory = MemorySaver()
        self.store = MemoryRetriver()

        self.system_messages = [
            """
    You are an assistant for BDI-Viz, a heatmap visualization tool designed for schema matching.
    Your role is to assist with schema matching operations and provide responses in a strict JSON schema format.
    Do not include any reasoning, apologies, or explanations in your responses.

    **Criteria for matching columns:**
    1. Column names and values do not need to be identical.
    2. Ignore case, special characters, and spaces.
    3. Columns should be considered a match if they are semantically similar and their values are comparable.
    4. Approach the task with the mindset of a biomedical expert.
            """,
        ]

    def search(self, query: str) -> SearchResponse:
        logger.info(f"[Agent] Searching for candidates...")

        tools = [
            self.store.query_candidates_tool,
        ]

        prompt = f"""
    Use the tools to search for candidates based on the user's input.

    User Query: {query}
        """

        logger.info(f"[SEARCH] Prompt: {prompt}")

        response = self.invoke(
            prompt=prompt,
            tools=tools,
            output_structure=SearchResponse,
        )

        return response

    def explain(self, candidate: Dict[str, Any]) -> CandidateExplanation:
        logger.info(f"[Agent] Explaining the candidate...")
        # logger.info(f"{diagnose}")

        # search for related false negative / false positive candidates
        related_matches = self.store.search_matches(candidate["sourceColumn"], limit=3)
        related_mismatches = self.store.search_mismatches(
            f"{candidate['sourceColumn']}::{candidate['targetColumn']}", limit=3
        )

        # search for related explanations
        related_explanations = self.store.search_explanations(
            f"{candidate['sourceColumn']}::{candidate['targetColumn']}", limit=3
        )

        prompt = f"""
    Analyze the following user operation details:

    - Source Column: {candidate["sourceColumn"]}
    - Target Column: {candidate["targetColumn"]}
    - Source Sample Values: {candidate["sourceValues"]}
    - Target Sample Values: {candidate["targetValues"]}

    Historical Data:
    - Related Matches: {related_matches}
    - Related Mismatches: {related_mismatches}
    - Related Explanations: {related_explanations}

    Instructions:
    1. Review the operation details alongside the historical data.
    2. Provide up to four possible explanations that justify whether the columns are a match or not. Reference the historical matches, mismatches, and explanations where relevant.
    3. Conclude if the current candidate is a valid match based on:
        a. Your explanations,
        b. Similarity between the column names,
        c. Consistency of the sample values, and
        d. The history of false positives and negatives.
    4. For categorical columns, suggest potential pairs of matching values based on the sample data. For numeric columns, omit this step.
    5. Include any additional context or keywords that might support or contradict the current mapping.
        """
        logger.info(f"[EXPLAIN] Prompt: {prompt}")
        response = self.invoke(
            prompt=prompt,
            tools=[],
            output_structure=CandidateExplanation,
        )
        return response

    def make_suggestion(
        self, explanations: List[Dict[str, Any]], user_operation: Dict[str, Any]
    ) -> AgentSuggestions:
        """
        Generate suggestions based on the user operation and diagnosis.

        Args:
            explanations (List[Dict[str, Any]]): A list of explanations to consider.
                [
                    {
                        'type': ExplanationType;
                        'content': string;
                        'confidence': number;
                    },
                    ...
                ]
            user_operation (Dict[str, Any]): The user operation to consider.
        """
        logger.info(f"[Agent] Making suggestion to the agent...")
        # logger.info(f"{diagnosis}")

        explanations_str = "\n".join(
            f"\tDiagnosis: {explanation['content']}, Confidence: {explanation['confidence']}"
            for explanation in explanations
        )
        user_operation_str = f"""
Operation: {user_operation["operation"]}
Candidate: {user_operation["candidate"]}
        """

        prompt = f"""
User Operation:
{user_operation_str}

Diagnosis:
{explanations_str}

**Instructions**:
    1. Generate 2-3 suggestions based on the user operation and diagnosis:
        - **undo**: Undo the last action if it seems incorrect.
        - **prune_candidates**: Suggest pruning candidates based on RAG expertise.
        - **update_embedder**: Recommend a more accurate model if matchings seem wrong.
    2. Provide a brief explanation for each suggestion.
    3. Include a confidence score for each suggestion.
        """

        logger.info(f"[SUGGESTION] Prompt: {prompt}")

        response = self.invoke(
            prompt=prompt,
            tools=[],
            output_structure=AgentSuggestions,
        )

        return response

    def apply(
        self, session: str, action: Dict[str, Any], previous_operation: Dict[str, Any]
    ) -> Optional[ActionResponse]:
        user_operation = previous_operation["operation"]
        candidate = previous_operation["candidate"]
        # references = previous_operation["references"]

        candidate_butler = CandidateButler(session)

        source_cluster = candidate_butler.read_source_cluster_details(
            candidate["sourceColumn"]
        )

        logger.info(f"[Agent] Applying the action: {action}")

        if action["action"] == "prune_candidates":
            tools = candidate_butler.get_toolset() + [retrieve_from_rag]
            prompt = f"""
You have access to the user's previous operations and the related source column clusters. 
Your goal is to help prune (remove) certain candidate mappings in the related source columns based on the user's decisions following the instructions below.

**Previous User Operation**:
Operation: {user_operation}
Candidate: {candidate}

**Related Source Columns and Their Candidates**:
{source_cluster}

**Instructions**:
1. Identify **Related Source Columns and Their Candidates**.
2. Consult Domain Knowledge (using **retrieve_from_rag**) if any clarifications are needed.
3. Decide Which Candidates to Prune based on your understanding and the userâ€™s previous operations, then compile the candidates after pruning into a **dictionary** like this:
    [
        {{"sourceColumn": "source_column_1", "targetColumn": "target_column_1", "score": 0.9, "matcher": "magneto_zs_bp"}},
        {{"sourceColumn": "source_column_1", "targetColumn": "target_column_15", "score": 0.7, "matcher": "magneto_zs_bp"}},
        ...
    ]
4. Call **update_candidates** with this updated dictionary as the parameter to refine the heatmap.
                """

            logger.info(f"[ACTION-PRUNE] Prompt: {prompt}")
            response = self.invoke(
                prompt=prompt,
                tools=tools,
                output_structure=ActionResponse,
            )
            return response

        elif action["action"] == "undo":
            return ActionResponse(
                status="success",
                response="Action successfully undone.",
                action="undo",
            )
        else:
            logger.info(f"[Agent] Applying the action: {action}")
            return

    def remember_fp(self, candidate: Dict[str, Any]) -> None:
        logger.info(f"[Agent] Remembering the false positive...")
        self.store.put_mismatch(candidate)

    def remember_fn(self, candidate: Dict[str, Any]) -> None:
        logger.info(f"[Agent] Remembering the false negative...")
        self.store.put_match(candidate)

    def remember_explanation(
        self, explanations: List[Dict[str, Any]], user_operation: Dict[str, Any]
    ) -> None:
        logger.info(f"[Agent] Remembering the explanation...")
        self.store.put_explanation(explanations, user_operation)

    def remember_candidates(self, candidates: List[Dict[str, Any]]) -> None:
        logger.info(f"[Agent] Remembering the candidates...")
        for candidate in candidates:
            self.store.put_candidate(candidate)

    def invoke(
        self, prompt: str, tools: List, output_structure: BaseModel
    ) -> BaseModel:
        output_parser = PydanticOutputParser(pydantic_object=output_structure)

        prompt = self.generate_prompt(prompt, output_parser)
        agent_executor = create_react_agent(
            self.llm, tools, store=self.store
        )  # checkpointer=self.memory

        responses = []
        for chunk in agent_executor.stream(
            {
                "messages": [
                    SystemMessage(content=self.system_messages[0]),
                    HumanMessage(content=prompt),
                ]
            },
            self.agent_config,
        ):
            logger.info(chunk)
            logger.info("----")
            responses.append(chunk)

        final_response = responses[-1]["agent"]["messages"][0].content
        response = output_parser.parse(final_response)

        return response

    def invoke_system(self, prompt: str) -> Generator[AIMessage, None, None]:
        agent_executor = create_react_agent(self.llm, store=self.store)
        for chunk in agent_executor.stream(
            {"messages": [SystemMessage(content=prompt)]}, self.agent_config
        ):
            logger.info(chunk)
            yield chunk

    def bind_tools(self, tools: List, tool_choice: Optional[str] = None) -> None:
        if tool_choice is not None:
            return self.llm.bind_tools(tools, tool_choice=tool_choice)
        else:
            logger.info(f"[Agent] Binding tools to the agent...")
            return self.llm.bind_tools(tools)

    def generate_prompt(self, prompt: str, output_parser: PydanticOutputParser) -> str:
        instructions = output_parser.get_format_instructions()
        template = f"""
Directly return the JSON in the exact schema described below. 
No extra text before or after the JSON.

{instructions}

Prompt: {prompt}
"""
        return template


AGENT = Agent()
